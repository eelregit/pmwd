\documentclass[usenatbib]{mnras}

% Allow "Thomas van Noord" and "Simon de Laguarde" and alike to be sorted by "N" and "L" etc. in the bibliography.
% Write the name in the bibliography as "\VAN{Noord}{Van}{van} Noord, Thomas"
\DeclareRobustCommand{\VAN}[3]{#2}
\let\VANthebibliography\thebibliography
\def\thebibliography{\DeclareRobustCommand{\VAN}[3]{##3}\VANthebibliography}


\usepackage{microtype}
\usepackage{newpxtext, eulerpx}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{nicefrac}


\newcommand{\pmwd}{{\usefont{T1}{nova}{m}{sl}pmwd}}

\renewcommand{\sectionautorefname}{Sec.}
\renewcommand{\subsectionautorefname}{Sec.}
\renewcommand{\appendixautorefname}{App.}
\renewcommand{\figureautorefname}{Fig.}
%\newcommand{\subfigureautorefname}{\figureautorefname}

%FIXME copied from ../adjoint/adjoint.tex
\newcommand{\deltaD}{\delta^\textsc{d}}
\newcommand{\deltaK}{\delta^\textsc{k}}
\renewcommand{\d}{d}
\newcommand{\p}{\partial}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cH}{\mathcal{H}}
\bmdefine{\vzero}{0}
\bmdefine{\vI}{I}
\bmdefine{\vnabla}{\nabla}
\bmdefine{\vtheta}{\theta}  % parameters
\bmdefine{\vvartheta}{\vartheta}  % relevant scales and factors to form
                                  % dimensionless features as NN inputs
\bmdefine{\vomega}{\omega}  % white noise modes
\bmdefine{\vk}{k}  % wavevectors
\bmdefine{\vx}{x}  % comoving and canonical coordinates
\bmdefine{\vq}{q}  % Lagrangian coordinates
\bmdefine{\vs}{s}  % displacements
\bmdefine{\vp}{p}  % canonical momenta
\bmdefine{\vv}{v}  % FIXME some velocities
\bmdefine{\va}{a}  % accelerations
\bmdefine{\vz}{z}  % states
\bmdefine{\vo}{o}  % observable
\bmdefine{\vf}{f}
\bmdefine{\vF}{F}
\bmdefine{\vDelta}{\Delta}
\bmdefine{\vLambda}{\Lambda}
\bmdefine{\vlambda}{\lambda}
\bmdefine{\vvarphi}{\varphi}
\bmdefine{\vxi}{\xi}  % x adjoint
\bmdefine{\vpi}{\pi}  % p adjoint
\bmdefine{\valpha}{\alpha}  % force vjp x gradient
\bmdefine{\vzeta}{\zeta}  % force vjp theta gradient
\newcommand{\half}{\nicefrac12}
\newcommand{\As}{A_\mathrm{s}}
\newcommand{\ns}{n_\mathrm{s}}
\newcommand{\Omegam}{\Omega_\mathrm{m}}
\newcommand{\Omegab}{\Omega_\mathrm{b}}
\newcommand{\Mpc}{\mathrm{Mpc}}
\newcommand{\ic}{\mathrm{i}}
\newcommand{\linear}{\mathrm{lin}}
\newcommand{\tophat}{\mathrm{TH}}
\newcommand{\gauss}{\mathrm{G}}

\newcommand{\YL}[1]{\textcolor{Bittersweet}{#1}}



\title[Spatiotemporally Optimized Simulation]
{Neural and Symbolic Spatiotemporal Optimization of Cosmological
Particle-Mesh Simulation}


\author[Zhang, Li, Jamieson, et al.]{
%
Yucheng Zhang,$^{1, 2}$
%
Yin Li,$^{1, 3}$\thanks{Email: XYZ, eelregit@gmail.com, and XYZ}
%
and Drew Jamieson$^{4}$
%
\\$^1$Department of Mathematics and Theory, Peng Cheng Laboratory,
Shenzhen, Guangdong 518066, China
%
\\$^2$Center for Cosmology and Particle Physics, Department of Physics,
New York University, New York, NY 10003, USA
%
\\$^3$Center for Computational Astrophysics \& Center for Computational
Mathematics, Flatiron Institute, New York, NY 10010, USA
%
\\$^4$Max Planck Institute for Astrophysics, 85748 Garching bei
M\"unchen, Germany
}


\date{Accepted XXX. Received YYY; in original form ZZZ}
\pubyear{2023}



\begin{document}
\label{firstpage}
\pagerange{\pageref{firstpage}--\pageref{lastpage}}
\maketitle



\begin{abstract}
We sharpen PM force and optimize integration time steps.
\end{abstract}

\begin{keywords}
cosmology: large-scale structure of Universe
-- methods: numerical
-- software: development
\end{keywords}



\section*{TODO}
\begin{itemize}
\item Separate sto to SO and TO.
\item Moving some of the following ongoing and completed tasks to the
    main text and add preliminary figures.
\item Implement the rest of the nonlinear scales, including the rms
    displacement with np.trapz of $P$ over $\ln k$, the $\ln a$ time
    derivatives, etc.\newline
    \textbf{Update}: code has been added.
\item Compare one pair of Gadget4 and \pmwd\ (with say 100 to 1000 time
    steps) results and make sure that they agree perfectly on the large
    scales. For this purpose the box size should be large enough (in
    linear regime), say 1 Gpc/$h$.\newline
    \textbf{Update}: for Sobol=6, boxsize=983Mpc, Gadget4 and \pmwd\ are visually
    consistent at the field level. To further check power spectrum. For odd mesh
    shapes (e.g. 1, 3, 5), pmwd and Gadget are perfectly consistent on large
    scales. \textbf{While for even mesh shapes (e.g. 2, 4), the powers look
    slightly different. To figure out this weird issue.}
\item Be careful with the non-deterministic ICs generated on GPUs.\newline
      \textbf{Update}: we have shown that ICs generated on GPUs are
      deterministic (but different from those on CPUs), generating ICs for both
      training data and training on GPUs might be the simplest solution.
\item Incorporate 3lpt branch; testing; find a\_start for different mass
  resolutions
\item Probably necessary to first incorporate List \& Hahn's recent
  improvement on FastPM integrator arXiv:2301:09655.
\item Compare sto DM and halo fields and summary stats with those of raw
  pmwd, deconvolved pmwd, and gadget, on the last 9 test cases.
\item Demonstrate capability of generalization, on some cases with
  extrapolating cosmological parameters, etc.
\item Non-integer multiple mesh shapes: for uniform particle grid, test
  (with double precision on CPUs) whether they start moving by artifact
  forces on cell scales, in 3 cases: no deconvolution, with CIC
  deconvolution, with SO.
\item In TO, time step spacing can be important for light cone
  generation. We can train on light cone target data to learn TO for
  light cones. The result may be related to $c \d t/H = c \d a / a^2 H$.
\end{itemize}


\section{Introduction}


\section{Methods}


\subsection{Simulation and Differentiation}

We briefly recap the discussion in \citet{Li2022a} and introduce the notations.
We denote the particles' state with $\vz = (\vx, \vp)^\intercal$ and their
adjoint variables with $\vlambda = (\vxi, \vpi)$.
The forward simulation is evolved using a symplectic integrator.
From $a_{i-1}$ to $a_i$, the particles' state is updated alternately for $\vx$
and $\vp$ through a loop over the symplectic integration indexed with $j$
\begin{alignat}{2}
&D_{j-1}^j: &\qquad \vx_j &= \vx_{j-1} +
  \vp_{j-1} D(a^p_{j-1}, a^x_{j-1}, a^x_j), \nonumber\\
%
&F_j: & \va_j &\triangleq -\vnabla\varphi(\vx_j), \nonumber\\
%
&K_{j-1}^j: & \vp_j &= \vp_{j-1} + \va_j K(a^x_j, a^p_{j-1}, a^p_j).
\label{DFK}
\end{alignat}
The intermediate times are given by
\begin{align}
a^x_j &= (1 - C^x_j) a_{i-1} + C^x_j a_i, \nonumber\\
a^p_j &= (1 - C^p_j) a_{i-1} + C^p_j a_i,
\end{align}
where $C^x$ and $C^p$ are the cumulative coefficients of the symplectic
integrator, e.g. $C^x = (0, 0, 1)$ and $C^p = (0, 0.5, 1)$ for leapfrog.
The reverse time integration from $a_i$ to $a_{i-1}$ and adjoint equations
\begin{alignat}{2}
&K_j^{j-1}: &\qquad \vp_{j-1} &= \vp_j +
  \va_j K(a^x_j, a^p_j, a^p_{j-1}), \nonumber\\
%
& & \vxi_{j-1} &= \vxi_j - \valpha_j K(a^x_j, a^p_j, a^p_{j-1}) \nonumber\\
%
&D_j^{j-1}: & \vx_{j-1} &= \vx_j +
  \vp_{j-1} D(a^p_{j-1}, a^x_j, a^x_{j-1}), \nonumber\\
%
& & \vpi_{j-1} &= \vpi_j - \vxi_{j-1} D(a^p_{j-1}, a^x_j, a^x_{j-1}), \nonumber\\
%
&F_{j-1}: & \va_{j-1} &\triangleq -\vnabla\vvarphi(\vx_{j-1}), \nonumber\\
%
& & \valpha_{j-1} &\triangleq -\vpi_{j-1} \cdot
  \frac{\p\vnabla\vvarphi_{j-1}}{\p\vx_{j-1}}, \nonumber\\
& & \vzeta_{j-1} &\triangleq - \vpi_{j-1} \cdot
  \frac{\p\vnabla\vvarphi_{j-1}}{\p\vtheta}.
\label{KDF_adj}
\end{alignat}

We consider an objective function $\cJ(\vo, \theta)$ defined on the observable
$\vo(\vz_0,...,\vz_n,\vtheta)$.
The observe operator from $a_i$ to $a_{i-1}$ reads
\begin{align}
  O_i^{i-1}: \qquad \vxi_{i-1} :\:\!\!\!&= \vxi_{i-1}
  + \frac{\p\cJ}{\p\vo} \cdot \frac{\p\vo}{\p\vx_{i-1}}, \nonumber\\
                    \vpi_{i-1} :\:\!\!\!&= \vpi_{i-1}
  + \frac{\p\cJ}{\p\vo} \cdot \frac{\p\vo}{\p\vp_{i-1}}.
\end{align}

% % TODO update the final gradient for loop over j
% The final gradient is accumulated through
% \begin{multline}
%   \frac{\d\cJ}{\d\vtheta} = \frac{\p\cJ}{\p\vtheta}
%   %
%   + \frac{\p\cJ}{\p\vo} \cdot \frac{\p\vo}{\p\vtheta}
%   %
%   + \vxi_0 \cdot \frac{\p\vx_0}{\p\vtheta}
%   + \vpi_0 \cdot \frac{\p\vp_0}{\p\vtheta} \\
%   %
%   - \sum_{i=1}^n \Bigl[
%     \bigl( \vpi_i \cdot\va_i \frac\p{\p\vtheta} + \vzeta_i
%       \bigr) K(t_i, t_i, t_{i-\half}) \\
%     + \vxi_i \cdot \vp_{i-\half}
%       \frac{\p D(t_{i-\half}, t_i, t_{i-1})}{\p\vtheta}
%     \Bigr],
% \label{KDFO_grad}
% \end{multline}
% where $\vtheta$ could include cosmological parameters, initial conditions, as
% well as STO parameters in this work.


\subsection{Snapshot Observable}
\label{sec:snapobs}

Given two snapshots at $a_i$ and $a_{i+1}$, we get the particle displacements
in between at $a$ using interpolation with the cubic Hermite spline,
\begin{align}
  \vx(a) =\ &h_{00}(\alpha)\vx_i + h_{10}(\alpha)(a_{i+1} - a_i)\vv_i + \nonumber\\
           &h_{01}(\alpha)\vx_{i+1} + h_{11}(\alpha)(a_{i+1} - a_i)\vv_{i+1},
\end{align}
where $\alpha := (a - a_i)/(a_{i+1} - a_i)$ and the Hermite basis functions read
\begin{align}
  h_{00}(\alpha) &= 2\alpha^3 - 3\alpha^2 + 1, \nonumber\\
  h_{10}(\alpha) &= \alpha^3 - 2\alpha^2 + \alpha, \nonumber\\
  h_{01}(\alpha) &= -2\alpha^3 + 3\alpha^2, \nonumber\\
  h_{11}(\alpha) &= \alpha^3 - \alpha^2.
\end{align}
The particle velocity is then given by the derivative $\d\vx / \d a$.

Our observable $\vo$ is a snapshot $\hat\vz = (\hat\vx, \hat\vp)^\intercal$ at a
given time $a_o$, which in general can be given by the interpolation of two
snapshots $\vz_{k-1}$ and $\vz_k$ where $a_{k-1} \leq a_o < a_k$.
Then ${\p\vo}/{\p\vz_i}$ would be zero except for the relevant $\vz_{k-1}$ and
$\vz_k$.
We have in the initial conditions of the adjoint equations
\begin{align}
  \vxi_n &= \frac{\p\cJ}{\p\vo} \cdot \frac{\p\vo}{\p\vx_n}, \nonumber\\
  \vpi_n &= \frac{\p\cJ}{\p\vo} \cdot \frac{\p\vo}{\p\vp_n}.
\end{align}
In the reverse time evolution of the adjoint equations, these adjoint varibales
remain zero until the time stepping from $a_{k+1}$ to $a_k$.


\subsection{Objective Function}

Again combine Lagrangian and Eulerian fields.

A mean squared error (MSE) loss on the fields would focus too much on
the small scales. In this work we choose uniform weights in the scale
space, by weighing each mode of wavenumber $k$ by $k^{-3}$ in the MSE
loss
\begin{equation}
  \cJ = \mathbb{E} \sum_\vk k^{-3}
    \bigl| \hat f(\vk) - f(\vk) \bigr|^2.
\end{equation}

An alternative loss function can be the Eulernian density weighted
particle displacement MSE.



\subsection{Spatial Optimization}
\label{sec:so}

The dimensionless power spectrum
%
\begin{equation}
\Delta^2(k) \triangleq \frac{k^3 P(k)}{2 \pi^2}.
\end{equation}

Variance of linear matter overdensity in a top-hat window
%
\begin{equation}
\sigma_\tophat^2(R) = \int_0^\infty \frac{\d k}k
  \Delta_\linear^2(k) W_\tophat^2(kR),
\end{equation}
where $W_\tophat(kR) = 3[\sin(kR) - kR\cos(kR)] / (kR)^3$ is the
Fourier transform of the top-hat window.
Likewise $\sigma_\gauss^2(R)$ is defined with a Gaussian window
$W_\gauss(kR) = e^{-(kR)^2/2}$.


Various nonlinear scales are
\begin{itemize}
\item $k_P^{-1}$: determined by $\Delta_\linear^2(k_P) \triangleq 1$;
\item $R_\tophat$: determined by $\sigma_\tophat^2(R_\tophat) \triangleq
  1$;
\item $R_\gauss$: determined by $\sigma_\gauss^2(R_\tophat) \triangleq
  1$;
\item $R_\mathrm{d}$: rms linear theory displacement;
\item $\d R / \d\ln a$: time derivatives of the above scales.
\end{itemize}

Other relevant scales are
\begin{itemize}
\item $l_\mathrm{cell}$: PM cell size.
\item particle spacing;
\item softening length.
\end{itemize}

Relevant scale-independent dimensionless factors are
\begin{itemize}
\item $G_m(a) \triangleq D_m(a) / a^m$: linear growth given in
  suppression factors for $m \in \{1, 2\}$;
\item $\d\ln G_m / \d\ln a$: with $m=1$ case related to the growth rate
  $f \triangleq \d\ln D_1 / \d\ln a$;
\item $\Omegam(a)$: matter density parameter at $a$;
\item $\d\ln H / \d\ln a$: time derivative of Hubble expansion;
\item $\Delta\ln a$: time step size in $\ln a$.
\end{itemize}

We combine the above list of parameters in $\vvartheta$.


In Fourier space, the gravitational force field reads
\begin{equation}
- i \vk \varphi(\vk) = i \frac32 \Omegam H_0^2 \,\frac{\vk}{k^2} \delta(\vk),
\end{equation}
Force sharpening
%
\begin{equation}
\frac{k_i}{k^2} \to \frac{k_i}{k^2}
  h(k_i; \vvartheta) g(k; \vvartheta) \prod_{j=1}^3 f(k_j; \vvartheta),
\end{equation}
%
where $f$, $g$, $h$ are neural networks, each approximating a nonlinear
function of the dimensionless features, e.g.,
%
\begin{equation}
g(k; \vvartheta) = g(k/k_P, kR_\tophat, \cdots; D, f, \cdots).
\end{equation}

Parameters in these neural networks are optimized during training.


\subsection{Temporal Optimization}
\label{sec:to}

We use a spline to model the function $a(t)$ that maps uniformly sampled $t$
values to optimally distributed $a$ time steps.
The spline is parameterized with a number of control points that are optimized
during training.
The gradients (i.e. cotangents) are back propagated from the array of
integration time steps to the control points through a VJP transformation of
$a(t)$.

The time-dependence of the discrete symplectic integrator mainly enters through
the drift and kick factors in \eqref{DFK}.
Besides, the input features of the SO NNs introduced in \autoref{sec:so} could
also depend on time.
Therefore when we perform TO together with SO, the force evaluation in
\eqref{DFK} which contains SO also becomes time-dependent and needs to be
optimized.


\begin{table*}
\centering
\caption{Ranges of GADGET-4 and \pmwd\ configuration and cosmological
parameters.
Note that the grid ratio need next fast len to determine the mesh shape
for fast FFT.
Given the box size, the mesh shape determines the cell size.
The softening parameter gives the ratio of the comoving softening length
to the mean particle spacing.
The curvature $\Omega_k$ is related to the separate universe simulation.
We sample parameters applicable to GADGET-4 during data generation, and
those applicable to \pmwd\ during training.
\textsuperscript\dag The box size is determined jointly by the \pmwd\
mesh shape and mesh cell size below, which are assumed to be sampled
independently.
In practice, we sample one conditioned on the other and the box size.
* Number of time steps from $a=1/16$ to $a=1$.
}
\label{tab:param}
\begin{tabular}{lcccr}
  \toprule
  applicability & parameter & distribution & range & sampling \\
  \midrule
  & $128^3$ white noise & $\mathcal{N}(\vzero, \vI)$ & 512 realizations & MC \\
  \cmidrule(lr){2-5}
  & 121 snapshots, $a$ & uniform & [1/16, 1] & deterministic \\
  \cmidrule(lr){2-5}
  & box size in Mpc\textsuperscript\dag & log-trapezoidal & [25.6, 2560) \\
  \cmidrule(lr){2-4}
  & snapshot offset, $\Delta\!a$ & uniform & [0, 1/128) \\
  \cmidrule(lr){2-4}
  GADGET-4 & $\As \times 10^9$ & log-uniform & [1, 4) \\
  \cmidrule(lr){2-4}
  \& \pmwd\ & $\ns$ & log-uniform & [0.75, 1.25) & RQMC \\
  \cmidrule(lr){2-4}
  & $\Omegam$ & log-uniform & [1/5, 1/2) & see \autoref{fig:sobol} \\
  \cmidrule(lr){2-4}
  & $\Omegab / \Omegam$ & log-uniform & [1/8, 1/4) \\
  \cmidrule(lr){2-4}
  & $\Omega_k / (1 - \Omega_k)$ & uniform & [-1/3, 1/3) \\
  \cmidrule(lr){2-4}
  & $h$ & log-uniform & [0.5, 1) \\
  \cmidrule(lr){1-4}
  GADGET-4 & softening ratio & log-uniform & [1/50, 1/20) \\
  \cmidrule(lr){1-5}
  & mesh shape & log-uniform & [128, 512] \\
  \cmidrule(lr){2-4}
  \pmwd\ & mesh cell size in Mpc & log-uniform & [0.2, 5] & MC \\
  \cmidrule(lr){2-4}
  & number of time steps* & log-uniform & [10, 1000] \\
  \bottomrule
\end{tabular}
\end{table*}


\subsection{Parameters and Configurations}

We run 512 GADGET-4 simulations that cover a wide range of
configurations and cosmological parameters, as listed in
\autoref{tab:param}.
Each simulation has $128^3$ particles and writes 121 snapshots linearly
spaced in scale factor from $1/16 + \Delta a$ to $1 + \Delta a$, with
spacing $1/128$.
$\Delta a$ is an offset, common for all snapshots in each simulation,
that varies as in \autoref{tab:param} and helps to sample different
alignment between snapshot and start/stop times.

The curvature $\Omega_k$ is related to the separate universe simulation
\citep{LiEtAl2014a, WagnerEtAl2015}.


\begin{figure*}
  \centering
  \includegraphics[width=0.9\linewidth]{sobol.pdf}
  \caption{Randomized Quasi-Monte Carlo (RQMC) configuration with
    scrambled Sobol sequence of 512 points in 9D.
    Lower triangular panels show the 2D projections and the diagonal
    panels are the 1D cumulative histograms.
    From left to right (top to bottom), we use each dimension of the
    sample to scale the parameters as ordered in \autoref{tab:param}.
    We use the \texttt{scipy.stats.qmc} package \citep{SciPy} to
    generate the Sobol sequence \citep{Sobol1967}, which uses the
    direction number from \citet{JoeKuo2008} and the Owen scrambling
    \citep{Owen1998}.
    We search among 65536 scrambling seeds to minimize the mixture
    discrepancy (a uniformity measure) proposed in \citet{Zhou2013MD}.
  }
  \label{fig:sobol}
\end{figure*}


\subsection{Simulations}

\citet{GADGET-4}

We configure GADGET-4 with high force and time integration accuracy
settings.
For gravitational force, we choose pure FMM with $p=5$ and opening angle
0.4.
%FIXME list other from Config.sh and param.txt


\subsection{Training}

Given 512 Gadget simulations and 121 output snapshots each, in total we have
61952 snapshots for training.
Following \autoref{tab:param}, given a snapshot from Gadget simulations, we
configure \pmwd\ with the same initial coniditions and cosmological parameters,
and randomly sample the mesh shape (or the mesh cell size) and the number of
time steps.

We perform parallel training using a number of nodes that each hosts multiple
GPU devices.
The training on a given snapshot is performed on a single GPU device.
The gradients are collected and averaged to update the STO parameters.
To make sure that each individual training in a parallel run finishes in similar
time, they share the same number of time steps.


\section{Results}

\subsection{SO function}

We use the \texttt{PySR} \citep{PySR, Cranmer2020} symbolic regression code to
fit for an analytic function expression of the SO neural network.



\section{Conclusions}


\section*{Acknowledgements}

YZ and YL were supported by The Major Key Project of PCL.
The Flatiron Institute is supported by the Simons Foundation.


\section*{Code Availability}

\pmwd\ \citep{Li2022b} is open-source on
\href{https://github.com/eelregit/pmwd}{GitHub}, including the
\href{https://github.com/eelregit/pmwd/tree/master/docs/papers/sto}{source
and scripts of this paper}.



\bibliographystyle{mnras}
\bibliography{sto}



\bsp  % typesetting comment
\label{lastpage}
\end{document}
